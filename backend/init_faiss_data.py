#!/usr/bin/env python3
"""
Script to initialize FAISS data from text files in the data directory
"""

import json
import os
import numpy as np
import asyncio
from sentence_transformers import SentenceTransformer
from database import add_to_faiss, get_faiss_index, save_faiss_data, init_db
from langchain_text_splitters import RecursiveCharacterTextSplitter

def load_text_files():
    """
    Load text files from data directory and split into chunks
    """
    data_dir = "../data"
    documents = []
    metadata = []
    
    # Text splitter configuration
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
    )
    
    # Process each text file
    for filename in os.listdir(data_dir):
        if filename.endswith('.txt'):
            file_path = os.path.join(data_dir, filename)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Split content into chunks
                chunks = text_splitter.split_text(content)
                
                # Create metadata for each chunk
                for i, chunk in enumerate(chunks):
                    if len(chunk.strip()) < 50:  # Skip very short chunks
                        continue
                    
                    documents.append(chunk.strip())
                    metadata.append({
                        "source": filename,
                        "category": "general",
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "uploaded_at": "2024-01-01T00:00:00.000000"  # Default timestamp
                    })
                
                print(f"âœ… Processed {filename}: {len(chunks)} chunks")
                
            except Exception as e:
                print(f"âŒ Error processing {filename}: {e}")
    
    print(f"ðŸ“Š Total: {len(documents)} documents, {len(metadata)} metadata entries")
    return documents, metadata

async def create_faiss_index():
    """
    Create FAISS index from text files
    """
    print("ðŸš€ Báº¯t Ä‘áº§u táº¡o FAISS index tá»« dá»¯ liá»‡u text...")
    
    # Initialize database first
    print("ðŸ“Š Khá»Ÿi táº¡o database...")
    try:
        await init_db()
        print("âœ… Database initialized")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o database: {e}")
        return False
    
    # Load documents from text files
    print("ðŸ“š Äang táº£i dá»¯ liá»‡u tá»« text files...")
    documents, metadata = load_text_files()
    
    if not documents or not metadata:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½")
        return False
    
    # Initialize embedding model
    print("ðŸ“š Khá»Ÿi táº¡o embedding model...")
    try:
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Embedding model ready")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o embedding model: {e}")
        return False
    
    # Generate embeddings
    print("ðŸ”„ Äang táº¡o embeddings...")
    try:
        embeddings = embedding_model.encode(documents, show_progress_bar=True)
        print(f"âœ… ÄÃ£ táº¡o {len(embeddings)} embeddings")
    except Exception as e:
        print(f"âŒ Lá»—i táº¡o embeddings: {e}")
        return False
    
    # Clear existing FAISS index
    print("ðŸ§¹ XÃ³a FAISS index cÅ©...")
    try:
        # Delete existing index file if it exists
        import faiss
        index_file = "faiss_data/faiss_index.bin"
        if os.path.exists(index_file):
            os.remove(index_file)
            print("âœ… ÄÃ£ xÃ³a index cÅ©")
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi xÃ³a index cÅ©: {e}")
    
    # Add to FAISS
    print("ðŸ“ ThÃªm dá»¯ liá»‡u vÃ o FAISS...")
    try:
        add_to_faiss(embeddings, documents, metadata)
        print("âœ… ÄÃ£ thÃªm dá»¯ liá»‡u vÃ o FAISS")
    except Exception as e:
        print(f"âŒ Lá»—i thÃªm dá»¯ liá»‡u vÃ o FAISS: {e}")
        return False
    
    # Verify
    try:
        final_index = get_faiss_index()
        print(f"âœ… FAISS index hoÃ n thÃ nh vá»›i {final_index.ntotal} vectors")
        
        # Test search
        print("ðŸ§ª Kiá»ƒm tra tÃ¬m kiáº¿m...")
        test_query = "tuyá»ƒn sinh"
        test_embedding = embedding_model.encode([test_query])
        D, I = final_index.search(test_embedding, 2)
        
        print(f"Káº¿t quáº£ tÃ¬m kiáº¿m cho '{test_query}':")
        for i, (distance, idx) in enumerate(zip(D[0], I[0])):
            print(f"  {i+1}. Document {idx}: {documents[idx][:100]}... (distance: {distance:.4f})")
        
        return True
        
    except Exception as e:
        print(f"âŒ Lá»—i kiá»ƒm tra index: {e}")
        return False

async def main():
    """
    Main function
    """
    print("ðŸ”§ Táº¡o FAISS Index tá»« dá»¯ liá»‡u text PTITHCM")
    print("=" * 50)
    
    success = await create_faiss_index()
    
    if success:
        print("\nðŸŽ‰ HoÃ n thÃ nh! FAISS index Ä‘Ã£ sáºµn sÃ ng cho RAG")
        print("Báº¡n cÃ³ thá»ƒ cháº¡y á»©ng dá»¥ng vá»›i: python main.py")
    else:
        print("\nâŒ Tháº¥t báº¡i! Vui lÃ²ng kiá»ƒm tra lá»—i vÃ  thá»­ láº¡i")

if __name__ == "__main__":
    asyncio.run(main()) 