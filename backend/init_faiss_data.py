#!/usr/bin/env python3
"""
Script to initialize FAISS data from text files in the data directory
"""

import json
import os
import numpy as np
import asyncio
from sentence_transformers import SentenceTransformer
from database import add_to_faiss, get_faiss_index, save_faiss_data, init_db
from langchain_text_splitters import RecursiveCharacterTextSplitter

def load_text_files():
    """
    Load text files from data directory and split into chunks
    """
    data_dir = "../data"
    documents = []
    metadata = []
    
    # Text splitter configuration
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
    )
    
    # Category mapping based on filename and content
    def determine_category(filename: str, content: str) -> str:
        filename_lower = filename.lower()
        content_lower = content.lower()
        # Check for admission-related content
        admission_keywords = [
            "tuyá»ƒn sinh", "Ä‘Äƒng kÃ½", "há»“ sÆ¡", "Ä‘iá»ƒm chuáº©n", "há»c phÃ­", 
            "thá»§ tá»¥c nháº­p há»c", "chá»‰ tiÃªu", "phÆ°Æ¡ng thá»©c tuyá»ƒn sinh",
            "xÃ©t tuyá»ƒn", "thi Ä‘Ã¡nh giÃ¡", "thi tá»‘t nghiá»‡p", "há»c bá»•ng"
        ]
        # Check for academic-related content
        academic_keywords = [
            "chÆ°Æ¡ng trÃ¬nh há»c", "mÃ´n há»c", "giáº£ng viÃªn", "lá»‹ch há»c", 
            "thá»i khÃ³a biá»ƒu", "Ä‘Ã o táº¡o", "ngÃ nh há»c", "ká»¹ thuáº­t pháº§n má»m",
            "há»‡ thá»‘ng thÃ´ng tin", "khoa há»c mÃ¡y tÃ­nh", "máº¡ng mÃ¡y tÃ­nh",
            "cÃ´ng nghá»‡ thÃ´ng tin", "an toÃ n thÃ´ng tin", "trÃ­ tuá»‡ nhÃ¢n táº¡o",
            "quy Ä‘á»‹nh sinh viÃªn", "quy cháº¿ há»c vá»¥", "nghÄ©a vá»¥ sinh viÃªn", "ná»™i quy giáº£ng Ä‘Æ°á»ng",
            "ná»™i quy lá»›p há»c", "ná»™i quy ra vÃ o cá»•ng", "thi cá»­", "khen thÆ°á»Ÿng", "ká»· luáº­t sinh viÃªn",
            "quy trÃ¬nh há»c táº­p", "quy Ä‘á»‹nh thi tá»‘t nghiá»‡p", "quy Ä‘á»‹nh báº£o vá»‡ Ä‘á»“ Ã¡n",
            "nghiÃªn cá»©u khoa há»c", "hoáº¡t Ä‘á»™ng há»c táº­p", "Ä‘iá»ƒm rÃ¨n luyá»‡n", "Ä‘iá»ƒm chuyÃªn cáº§n",
            "quy Ä‘á»‹nh vá» trang phá»¥c", "quy Ä‘á»‹nh vá» sá»­ dá»¥ng tÃ i sáº£n trÆ°á»ng",
            "quy Ä‘á»‹nh vá» báº£o vá»‡ mÃ´i trÆ°á»ng", "quy Ä‘á»‹nh vá» an ninh tráº­t tá»±",
            "quy Ä‘á»‹nh vá» Ä‘Ã³ng há»c phÃ­", "quy Ä‘á»‹nh vá» nghá»‰ há»c", "quy Ä‘á»‹nh vá» báº£o lÆ°u káº¿t quáº£",
            "quy Ä‘á»‹nh vá» chuyá»ƒn ngÃ nh, chuyá»ƒn trÆ°á»ng", "quy Ä‘á»‹nh vá» xÃ©t tá»‘t nghiá»‡p",
            "quy Ä‘á»‹nh vá» há»c láº¡i, thi láº¡i", "quy Ä‘á»‹nh vá» há»c bá»•ng, há»— trá»£ sinh viÃªn"
        ]
        # Check filename first
        if "tuyen_sinh" in filename_lower:
            return "admission"
        elif "so_tay" in filename_lower:
            return "academic"
        elif "gioi_thieu" in filename_lower:
            return "general"
        # Check content keywords
        admission_count = sum(1 for keyword in admission_keywords if keyword in content_lower)
        academic_count = sum(1 for keyword in academic_keywords if keyword in content_lower)
        # Determine category based on keyword frequency
        if admission_count > academic_count:
            return "admission"
        elif academic_count > 0:
            return "academic"
        else:
            return "general"
    
    # Process each text file
    for filename in os.listdir(data_dir):
        if filename.endswith('.txt'):
            file_path = os.path.join(data_dir, filename)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Split content into chunks
                chunks = text_splitter.split_text(content)
                
                # Create metadata for each chunk
                for i, chunk in enumerate(chunks):
                    if len(chunk.strip()) < 50:  # Skip very short chunks
                        continue
                    
                    # GÃ¡n category cho tá»«ng chunk dá»±a trÃªn ná»™i dung chunk
                    chunk_category = determine_category(filename, chunk)
                    documents.append(chunk.strip())
                    metadata.append({
                        "source": filename,
                        "category": chunk_category,
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "uploaded_at": "2024-01-01T00:00:00.000000"  # Default timestamp
                    })
                
                print(f"âœ… Processed {filename}: {len(chunks)} chunks (category by chunk)")
                
            except Exception as e:
                print(f"âŒ Error processing {filename}: {e}")
    
    # Print category statistics
    category_counts = {}
    for meta in metadata:
        cat = meta.get("category", "unknown")
        category_counts[cat] = category_counts.get(cat, 0) + 1
    
    print(f"ðŸ“Š Category distribution:")
    for category, count in category_counts.items():
        print(f"  - {category}: {count} chunks")
    
    print(f"ðŸ“Š Total: {len(documents)} documents, {len(metadata)} metadata entries")
    return documents, metadata

async def create_faiss_index():
    """
    Create FAISS index from text files
    """
    print("ðŸš€ Báº¯t Ä‘áº§u táº¡o FAISS index tá»« dá»¯ liá»‡u text...")
    
    # Initialize database first
    print("ðŸ“Š Khá»Ÿi táº¡o database...")
    try:
        await init_db()
        print("âœ… Database initialized")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o database: {e}")
        return False
    
    # Load documents from text files
    print("ðŸ“š Äang táº£i dá»¯ liá»‡u tá»« text files...")
    documents, metadata = load_text_files()
    
    if not documents or not metadata:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ xá»­ lÃ½")
        return False
    
    # Initialize embedding model
    print("ðŸ“š Khá»Ÿi táº¡o embedding model...")
    try:
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Embedding model ready")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o embedding model: {e}")
        return False
    
    # Generate embeddings
    print("ðŸ”„ Äang táº¡o embeddings...")
    try:
        embeddings = embedding_model.encode(documents, show_progress_bar=True)
        print(f"âœ… ÄÃ£ táº¡o {len(embeddings)} embeddings")
    except Exception as e:
        print(f"âŒ Lá»—i táº¡o embeddings: {e}")
        return False
    
    # Clear existing FAISS index and data
    print("ðŸ§¹ XÃ³a FAISS index vÃ  dá»¯ liá»‡u cÅ©...")
    try:
        index_file = "faiss_data/faiss_index.bin"
        documents_file = "faiss_data/documents.json"
        metadata_file = "faiss_data/metadata.json"
        for f in [index_file, documents_file, metadata_file]:
            if os.path.exists(f):
                os.remove(f)
                print(f"âœ… ÄÃ£ xÃ³a {f}")
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi xÃ³a dá»¯ liá»‡u cÅ©: {e}")
    # Add to FAISS (reset)
    print("ðŸ“ ThÃªm dá»¯ liá»‡u vÃ o FAISS...")
    try:
        add_to_faiss(embeddings, documents, metadata, reset=True)
        print("âœ… ÄÃ£ thÃªm dá»¯ liá»‡u vÃ o FAISS")
    except Exception as e:
        print(f"âŒ Lá»—i thÃªm dá»¯ liá»‡u vÃ o FAISS: {e}")
        return False
    
    # Verify
    try:
        final_index = get_faiss_index()
        print(f"âœ… FAISS index hoÃ n thÃ nh vá»›i {final_index.ntotal} vectors")
        
        # Test search
        print("ðŸ§ª Kiá»ƒm tra tÃ¬m kiáº¿m...")
        test_query = "tuyá»ƒn sinh"
        test_embedding = embedding_model.encode([test_query])
        D, I = final_index.search(test_embedding, 2)
        
        print(f"Káº¿t quáº£ tÃ¬m kiáº¿m cho '{test_query}':")
        for i, (distance, idx) in enumerate(zip(D[0], I[0])):
            if idx < len(documents):  # Kiá»ƒm tra index bounds
                print(f"  {i+1}. Document {idx}: {documents[idx][:100]}... (distance: {distance:.4f})")
            else:
                print(f"  {i+1}. Document {idx}: [INDEX OUT OF RANGE] (distance: {distance:.4f})")
        
        return True
        
    except Exception as e:
        print(f"âŒ Lá»—i kiá»ƒm tra index: {e}")
        return False

async def main():
    """
    Main function
    """
    print("ðŸ”§ Táº¡o FAISS Index tá»« dá»¯ liá»‡u text PTITHCM")
    print("=" * 50)
    
    success = await create_faiss_index()
    
    if success:
        print("\nðŸŽ‰ HoÃ n thÃ nh! FAISS index Ä‘Ã£ sáºµn sÃ ng cho RAG")
        print("Báº¡n cÃ³ thá»ƒ cháº¡y á»©ng dá»¥ng vá»›i: python main.py")
    else:
        print("\nâŒ Tháº¥t báº¡i! Vui lÃ²ng kiá»ƒm tra lá»—i vÃ  thá»­ láº¡i")

if __name__ == "__main__":
    asyncio.run(main()) 