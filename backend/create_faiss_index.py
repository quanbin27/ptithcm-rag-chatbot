#!/usr/bin/env python3
"""
Script to create FAISS index from documents in faiss_data directory
"""

import json
import os
import numpy as np
import asyncio
from sentence_transformers import SentenceTransformer
from database import add_to_faiss, get_faiss_index, save_faiss_data, init_db

def load_documents():
    """
    Load documents and metadata from faiss_data directory
    """
    documents_path = "faiss_data/documents.json"
    metadata_path = "faiss_data/metadata.json"
    
    if not os.path.exists(documents_path) or not os.path.exists(metadata_path):
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file documents.json hoáº·c metadata.json")
        print("ğŸ’¡ Vui lÃ²ng cháº¡y init_faiss_data.py trÆ°á»›c Ä‘á»ƒ táº¡o dá»¯ liá»‡u")
        return None, None
    
    try:
        with open(documents_path, 'r', encoding='utf-8') as f:
            documents = json.load(f)
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Validate data consistency
        if len(documents) != len(metadata):
            print(f"âŒ Sá»‘ lÆ°á»£ng documents ({len(documents)}) khÃ´ng khá»›p vá»›i metadata ({len(metadata)})")
            return None, None
        
        print(f"âœ… ÄÃ£ táº£i {len(documents)} documents vÃ  {len(metadata)} metadata")
        return documents, metadata
        
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u: {e}")
        return None, None

async def create_faiss_index():
    """
    Create FAISS index from documents
    """
    print("ğŸš€ Báº¯t Ä‘áº§u táº¡o FAISS index...")
    
    # Initialize database first
    print("ğŸ“Š Khá»Ÿi táº¡o database...")
    try:
        await init_db()
        print("âœ… Database initialized")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o database: {e}")
        return False
    
    # Load documents
    documents, metadata = load_documents()
    if not documents or not metadata:
        return False
    
    # Validate document format
    if not isinstance(documents, list) or not all(isinstance(doc, str) for doc in documents):
        print("âŒ Documents khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng (pháº£i lÃ  list of strings)")
        return False
    
    # Validate consistency - use documents count as reference
    if len(documents) != len(metadata):
        print(f"âš ï¸ Data inconsistency: {len(documents)} documents vs {len(metadata)} metadata")
        print(f"âš ï¸ Using documents count ({len(documents)}) as reference")
        if len(metadata) > len(documents):
            metadata = metadata[:len(documents)]
            print(f"âš ï¸ Trimmed metadata to {len(metadata)} entries")
        else:
            print("âŒ Metadata count is less than documents, cannot proceed")
            return False
    
    # Initialize embedding model
    print("ğŸ“š Khá»Ÿi táº¡o embedding model...")
    try:
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Embedding model ready")
    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o embedding model: {e}")
        return False
    
    # Generate embeddings
    print("ğŸ”„ Äang táº¡o embeddings...")
    try:
        embeddings = embedding_model.encode(documents, show_progress_bar=True)
        print(f"âœ… ÄÃ£ táº¡o {len(embeddings)} embeddings vá»›i shape {embeddings.shape}")
    except Exception as e:
        print(f"âŒ Lá»—i táº¡o embeddings: {e}")
        return False
    
    # Clear existing FAISS index
    print("ğŸ§¹ XÃ³a FAISS index cÅ©...")
    try:
        # Delete existing index file if it exists
        import faiss
        index_file = "faiss_data/faiss_index.bin"
        if os.path.exists(index_file):
            os.remove(index_file)
            print("âœ… ÄÃ£ xÃ³a index cÅ©")
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi xÃ³a index cÅ©: {e}")
    
    # Add to FAISS
    print("ğŸ“ ThÃªm dá»¯ liá»‡u vÃ o FAISS...")
    try:
        add_to_faiss(embeddings, documents, metadata)
        print("âœ… ÄÃ£ thÃªm dá»¯ liá»‡u vÃ o FAISS")
    except Exception as e:
        print(f"âŒ Lá»—i thÃªm dá»¯ liá»‡u vÃ o FAISS: {e}")
        return False
    
    # Verify
    try:
        final_index = get_faiss_index()
        print(f"âœ… FAISS index hoÃ n thÃ nh vá»›i {final_index.ntotal} vectors")
        
        # Test search
        print("ğŸ§ª Kiá»ƒm tra tÃ¬m kiáº¿m...")
        test_query = "tuyá»ƒn sinh"
        test_embedding = embedding_model.encode([test_query])
        D, I = final_index.search(test_embedding, 2)
        
        print(f"Káº¿t quáº£ tÃ¬m kiáº¿m cho '{test_query}':")
        for i, (distance, idx) in enumerate(zip(D[0], I[0])):
            if idx < len(documents):
                print(f"  {i+1}. Document {idx}: {documents[idx][:100]}... (distance: {distance:.4f})")
        
        return True
        
    except Exception as e:
        print(f"âŒ Lá»—i kiá»ƒm tra index: {e}")
        return False

async def main():
    """
    Main function
    """
    print("ğŸ”§ Táº¡o FAISS Index tá»« dá»¯ liá»‡u PTITHCM")
    print("=" * 50)
    
    success = await create_faiss_index()
    
    if success:
        print("\nğŸ‰ HoÃ n thÃ nh! FAISS index Ä‘Ã£ sáºµn sÃ ng cho RAG")
        print("Báº¡n cÃ³ thá»ƒ cháº¡y á»©ng dá»¥ng vá»›i: python main.py")
    else:
        print("\nâŒ Tháº¥t báº¡i! Vui lÃ²ng kiá»ƒm tra lá»—i vÃ  thá»­ láº¡i")

if __name__ == "__main__":
    asyncio.run(main()) 